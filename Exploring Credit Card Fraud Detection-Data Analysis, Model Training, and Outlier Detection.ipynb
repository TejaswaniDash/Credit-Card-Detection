{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the dataset\n",
    "dataset = pd.read_csv(\"/Users/tejaswanidash/Desktop/Projects/Creditcard Fraud/creditcard.csv\")\n",
    "# read the first 5 and last 5 rows of the data\n",
    "dataset.head().append(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent Cases: 492\n",
      "Valid Transactions: 284315\n",
      "Proportion of Fraudulent Cases: 0.001727485630620034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel=' '>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Vizualization\n",
    "# check for relative proportion \n",
    "print(\"Fraudulent Cases: \" + str(len(dataset[dataset[\"Class\"] == 1])))\n",
    "print(\"Valid Transactions: \" + str(len(dataset[dataset[\"Class\"] == 0])))\n",
    "print(\"Proportion of Fraudulent Cases: \" + str(len(dataset[dataset[\"Class\"] == 1])/ dataset.shape[0]))\n",
    "\n",
    "# To see how small are the number of Fraud transactions\n",
    "data_p = dataset.copy()\n",
    "data_p[\" \"] = np.where(data_p[\"Class\"] == 1 ,  \"Fraud\", \"Genuine\")\n",
    "\n",
    "# plot a pie chart\n",
    "data_p[\" \"].value_counts().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-07016c565507>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-07016c565507>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Fraudulent Cases: 492\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Fraudulent Cases: 492\n",
    "Valid Transactions: 284315\n",
    "Proportion of Fraudulent Cases: 0.001727485630620034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the named features \n",
    "f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)\n",
    "\n",
    "amount_value = dataset['Amount'].values # values\n",
    "time_value = dataset['Time'].values # values\n",
    "\n",
    "sns.distplot(amount_value, hist=False, color=\"m\", kde_kws={\"shade\": True}, ax=axes[0]).set_title('Distribution of Amount')\n",
    "sns.distplot(time_value, hist=False, color=\"m\", kde_kws={\"shade\": True}, ax=axes[1]).set_title('Distribution of Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Amount in a Fraudulent Transaction: \" + str(dataset[dataset[\"Class\"] == 1][\"Amount\"].mean()))\n",
    "print(\"Average Amount in a Valid Transaction: \" + str(dataset[dataset[\"Class\"] == 0][\"Amount\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of the feature - Amount\" + \"\\n-------------------------------\")\n",
    "print(dataset[\"Amount\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns Amount, Time then the rest\n",
    "data_plot = dataset.copy()\n",
    "amount = data_plot['Amount']\n",
    "data_plot.drop(labels=['Amount'], axis=1, inplace = True)\n",
    "data_plot.insert(0, 'Amount', amount)\n",
    "\n",
    "# Plot the distributions of the features\n",
    "columns = data_plot.iloc[:,0:30].columns\n",
    "plt.figure(figsize=(12,30*4))\n",
    "grids = gridspec.GridSpec(30, 1)\n",
    "for grid, index in enumerate(data_plot[columns]):\n",
    " ax = plt.subplot(grids[grid])\n",
    " sns.distplot(data_plot[index][data_plot.Class == 1], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    " sns.distplot(data_plot[index][data_plot.Class == 0], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    " ax.set_xlabel(\"\")\n",
    " ax.set_title(\"Distribution of Column: \"  + str(index))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "# check for null values\n",
    "dataset.isnull().shape[0]\n",
    "print(\"Non-missing values: \" + str(dataset.isnull().shape[0]))\n",
    "print(\"Missing values: \" + str(dataset.shape[0] - dataset.isnull().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler().fit(dataset[[\"Time\", \"Amount\"]])\n",
    "dataset[[\"Time\", \"Amount\"]] = scaler.transform(dataset[[\"Time\", \"Amount\"]])\n",
    "\n",
    "dataset.head().append(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate response and features  Undersampling before cross validation will lead to overfiting\n",
    "y = dataset[\"Class\"] # target \n",
    "X = dataset.iloc[:,0:30]\n",
    "\n",
    "# Use SKLEARN for the split\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "        X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross validation framework \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the imbalance Learn module\n",
    "from imblearn.pipeline import make_pipeline ## Create a Pipeline using the provided estimators .\n",
    "from imblearn.under_sampling import NearMiss  ## perform Under-sampling  based on NearMiss methods. \n",
    "from imblearn.over_sampling import SMOTE  ## PerformOver-sampling class that uses SMOTE. \n",
    "# import the metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score\n",
    "# Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "\n",
    "# Fit and predict\n",
    "rfc = RandomForestClassifier() \n",
    "rfc.fit(X_train, y_train) \n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# For the performance let's use some metrics from SKLEARN module\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "  \n",
    "print(\"The accuracy is\", accuracy_score(y_test, y_pred)) \n",
    "print(\"The precision is\", precision_score(y_test, y_pred))\n",
    "print(\"The recall is\", recall_score(y_test, y_pred))\n",
    "print(\"The F1 score is\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_best_estimator_and_metrics(estimator, params, kf=kf, X_train=X_train, \n",
    "                                         y_train=y_train, X_test=X_test, \n",
    "                                         y_test=y_test, is_grid_search=True, \n",
    "                                         sampling=NearMiss(), scoring=\"f1\", \n",
    "                                         n_jobs=2):\n",
    "    if sampling is None:\n",
    "        # make the pipeline of only the estimator, just so the remaining code will work fine\n",
    "        pipeline = make_pipeline(estimator)\n",
    "    else:\n",
    "        # make the pipeline of over/undersampling and estimator\n",
    "        pipeline = make_pipeline(sampling, estimator)\n",
    "    # get the estimator name\n",
    "    estimator_name = estimator.__class__.__name__.lower()\n",
    "    # construct the parameters for grid/random search cv\n",
    "    new_params = {f'{estimator_name}__{key}': params[key] for key in params}\n",
    "    if is_grid_search:\n",
    "        # grid search instead of randomized search\n",
    "        search = GridSearchCV(pipeline, param_grid=new_params, cv=kf, return_train_score=True, n_jobs=n_jobs, verbose=2)\n",
    "    else:\n",
    "        # randomized search\n",
    "        search = RandomizedSearchCV(pipeline, param_distributions=new_params, \n",
    "                                    cv=kf, scoring=scoring, return_train_score=True,\n",
    "                                    n_jobs=n_jobs, verbose=1)\n",
    "    # fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    cv_score = cross_val_score(search, X_train, y_train, scoring=scoring, cv=kf)\n",
    "    # make predictions on the test data\n",
    "    y_pred = search.best_estimator_.named_steps[estimator_name].predict(X_test)\n",
    "    # calculate the metrics: recall, accuracy, F1 score, etc.\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    y_proba = search.best_estimator_.named_steps[estimator_name].predict_proba(X_test)[::, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    # return the best estimator along with the metrics\n",
    "    return {\n",
    "        \"best_estimator\": search.best_estimator_,\n",
    "        \"estimator_name\": estimator_name,\n",
    "        \"cv_score\": cv_score,\n",
    "        \"recall\": recall,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"auc\": auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "## Create the dataframe\n",
    "res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "logreg_us_results = get_model_best_estimator_and_metrics(\n",
    "    estimator=LogisticRegression(),\n",
    "    params={\"penalty\": ['l1', 'l2'], \n",
    "                  'C': [ 0.01, 0.1, 1, 100], \n",
    "                  'solver' : ['liblinear']},\n",
    "    sampling=NearMiss(),\n",
    ")\n",
    "print(f\"==={logreg_us_results['estimator_name']}===\")\n",
    "print(\"Model:\", logreg_us_results['best_estimator'])\n",
    "print(\"Accuracy:\", logreg_us_results['accuracy'])\n",
    "print(\"Recall:\", logreg_us_results['recall'])\n",
    "print(\"F1 Score:\", logreg_us_results['f1_score'])\n",
    "res_table = res_table.append({'classifiers': logreg_us_results[\"estimator_name\"],\n",
    "                                        'fpr': logreg_us_results[\"fpr\"], \n",
    "                                        'tpr': logreg_us_results[\"tpr\"], \n",
    "                                        'auc': logreg_us_results[\"auc\"]\n",
    "                              }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for undersampling\n",
    "res_table.set_index('classifiers', inplace=True)\n",
    "fig = plt.figure(figsize=(17,7))\n",
    "\n",
    "for j in res_table.index:\n",
    "    plt.plot(res_table.loc[j]['fpr'], \n",
    "             res_table.loc[j]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(j, res_table.loc[j]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"Positive Rate(False)\", fontsize=15)\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"Positive Rate(True)\", fontsize=15)\n",
    "plt.title('Analysis for Oversampling', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulatively create a table for the ROC curve\n",
    "res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "lin_reg_os_results = get_model_best_estimator_and_metrics(\n",
    "    estimator=LogisticRegression(),\n",
    "    params={\"penalty\": ['l1', 'l2'], 'C': [ 0.01, 0.1, 1, 100, 100], \n",
    "            'solver' : ['liblinear']},\n",
    "    sampling=SMOTE(random_state=42),\n",
    "    scoring=\"f1\",\n",
    "    is_grid_search=False,\n",
    "    n_jobs=2,\n",
    ")\n",
    "print(f\"==={lin_reg_os_results['estimator_name']}===\")\n",
    "print(\"Model:\", lin_reg_os_results['best_estimator'])\n",
    "print(\"Accuracy:\", lin_reg_os_results['accuracy'])\n",
    "print(\"Recall:\", lin_reg_os_results['recall'])\n",
    "print(\"F1 Score:\", lin_reg_os_results['f1_score'])\n",
    "res_table = res_table.append({'classifiers': lin_reg_os_results[\"estimator_name\"],\n",
    "                                        'fpr': lin_reg_os_results[\"fpr\"], \n",
    "                                        'tpr': lin_reg_os_results[\"tpr\"], \n",
    "                                        'auc': lin_reg_os_results[\"auc\"]\n",
    "                              }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detion and Removal\n",
    "\n",
    "# boxplot for two example variables in the dataset\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)\n",
    "\n",
    "variable1 = dataset[\"V1\"]\n",
    "variable2 = dataset[\"V2\"]\n",
    "\n",
    "sns.boxplot(variable1, color=\"m\", ax=axes[0]).set_title('Boxplot for V1')\n",
    "sns.boxplot(variable2, color=\"m\", ax=axes[1]).set_title('Boxplot for V2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the IQR for all the feature variables\n",
    "# Please note that we are keeping Class variable also in this evaluation, though we know using this method no observation\n",
    "# be removed based on this variable.\n",
    "\n",
    "quartile1 = dataset.quantile(0.25)\n",
    "quartile3 = dataset.quantile(0.75)\n",
    "\n",
    "IQR = quartile3 - quartile1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outliers \n",
    "constant = 3\n",
    "datavalid = dataset[~((dataset < (quartile1 - constant * IQR)) |(dataset > (quartile3 + constant * IQR))).any(axis=1)]\n",
    "deletedrows = dataset.shape[0] - datavalid.shape[0]\n",
    "print(\"We have removed \" + str(deletedrows) + \" rows from the data as outliers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
